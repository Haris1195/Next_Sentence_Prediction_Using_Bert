# -*- coding: utf-8 -*-
"""PRETRAINED  BERT MODEL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h5NoDPl5H9UskAe9s9BV7poYdFQ8QseM
"""

! pip install -U tokenizers

!pip install tensorflow==1.15

!mkdir data

cd data

from google.colab import files
upload=files.upload()

import tokenizers

vocab_length = 30522

roman_BWPT = tokenizers.BertWordPieceTokenizer(
      #add_special_tokens=True, # This argument doesn't work in the latest version of BertWordPieceTokenizer
     unk_token='[UNK]',
     sep_token='[SEP]',
     cls_token='[CLS]',
     clean_text=True,
     handle_chinese_chars=True,
     strip_accents=True,
     lowercase=True,
     wordpieces_prefix='##'
)
roman_BWPT.train(
    files=["/content/data/hi_dedup_1000.txt"],
    vocab_size=vocab_length,
     min_frequency=3,
     limit_alphabet=1000,
     show_progress=True,
     special_tokens=['[PAD]', '[UNK]', '[CLS]', '[MASK]', '[SEP]']
 )
 
roman_BWPT.save_model(".", "hindi"+str(vocab_length))

!python create_pretraining_data.py \
    --input_file hi_dedup_1000.txt \
    --output_file tf_examples_multi.tfrecord \
    --vocab_file hindi30522-vocab.txt \
    --do_lower_case True \
    --max_seq_length 128 \
    --max_predictions_per_seq 20 \
    --masked_lm_prob 0.15 \
    --random_seed 42 \
    --dupe_factor 5

!python run_pretraining.py \
    --input_file=tf_examples_multi.tfrecord* \
    --output_dir=tf_examples_multi \
    -do_train=True \
    --do_eval=True \
    --bert_config_file=config.json \
    --train_batch_size=32 \
    --max_seq_length=128 \
    --max_predictions_per_seq=20 \
    --num_train_steps=20 \
    --num_warmup_steps=10 \
    --learning_rate=2e-5 \

#

